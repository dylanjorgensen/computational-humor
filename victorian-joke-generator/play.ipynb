{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "Data size 17005207\n",
      "Most common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n",
      "Sample data [5238, 3082, 12, 6, 195, 2, 3136, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "3082 originated -> 12 as\n",
      "3082 originated -> 5238 anarchism\n",
      "12 as -> 3082 originated\n",
      "12 as -> 6 a\n",
      "6 a -> 12 as\n",
      "6 a -> 195 term\n",
      "195 term -> 6 a\n",
      "195 term -> 2 of\n",
      "Initialized\n",
      "Average loss at step  0 :  281.077911377\n",
      "Nearest to some: dreamland, josaphat, radial, propanol, schaff, watterson, courtyards, greet,\n",
      "Nearest to six: aphorism, macrae, craze, withdrew, riso, mcvie, concentrating, gracefully,\n",
      "Nearest to or: dictatorship, garc, areal, conjecture, laminar, courteous, dtv, lomond,\n",
      "Nearest to see: oversaw, dilmun, baseball, wheatstone, arrests, transactional, write, chivalrous,\n",
      "Nearest to only: mycenaean, lynched, clu, devi, alfredo, playground, meteorologist, stresses,\n",
      "Nearest to eight: shy, carnegie, lessons, dalriada, protruding, likes, sausage, discharges,\n",
      "Nearest to up: kaolinite, categorization, lautering, pago, abstracted, rouen, ashland, consult,\n",
      "Nearest to th: viv, cognomen, microscopy, sorcerers, stagnation, submit, distributism, secretly,\n",
      "Nearest to while: tempus, resumes, oblivion, customary, posters, earns, responsibility, pinto,\n",
      "Nearest to if: diss, antlia, bo, kets, internecine, tigers, altruists, kyoto,\n",
      "Nearest to their: inhabited, unnecessarily, uncivilized, touches, benefit, ej, appointed, fireplace,\n",
      "Nearest to on: antithesis, vit, china, equivalency, dominates, back, alienating, gaon,\n",
      "Nearest to these: vasoconstriction, etymological, boldface, control, odegard, statistical, mariani, holliday,\n",
      "Nearest to years: tude, barclay, presbyters, mutiny, pigmented, storming, roo, occuring,\n",
      "Nearest to b: contention, unlimited, mirrored, corneille, laren, holyoke, wrestlers, eyesight,\n",
      "Nearest to by: supposed, deflation, aylwin, pointed, abdu, christians, samson, hourglass,\n",
      "Average loss at step  2000 :  114.049429085\n",
      "Average loss at step  4000 :  52.6029954014\n",
      "Average loss at step  6000 :  33.3152458951\n",
      "Average loss at step  8000 :  23.6694422412\n",
      "Average loss at step  10000 :  17.9569663131\n",
      "Nearest to some: one, reginae, improvement, rico, genre, insertion, victoriae, these,\n",
      "Nearest to six: zero, agave, nine, three, eight, reginae, victoriae, vs,\n",
      "Nearest to or: and, vs, six, UNK, was, cc, choose, agave,\n",
      "Nearest to see: oversaw, baseball, vs, write, victoriae, neutrons, austin, returned,\n",
      "Nearest to only: implicit, stresses, mathbf, view, choices, brighton, treatments, band,\n",
      "Nearest to eight: nine, victoriae, vs, zero, phi, austin, reginae, succeeds,\n",
      "Nearest to up: reginae, tora, pago, categorization, polar, overview, vs, gravitational,\n",
      "Nearest to th: nine, microscopy, grandparents, basins, vs, absurd, exports, satellites,\n",
      "Nearest to while: reginae, and, crew, deliberate, exist, violent, keep, customary,\n",
      "Nearest to if: try, fields, studies, kyoto, reginae, artifacts, vs, nine,\n",
      "Nearest to their: appointed, the, benefit, inhabited, carbine, winged, pahlavi, victoriae,\n",
      "Nearest to on: in, and, one, china, back, with, blank, vs,\n",
      "Nearest to these: group, austria, control, piles, etymological, some, sense, statistical,\n",
      "Nearest to years: fao, zero, launch, saints, reginae, backed, tours, convictions,\n",
      "Nearest to b: unlimited, ages, contention, victoriae, psi, phi, take, four,\n",
      "Nearest to by: in, and, as, gb, albania, austin, of, UNK,\n",
      "Average loss at step  12000 :  13.8655570068\n",
      "Average loss at step  14000 :  11.5725150011\n",
      "Average loss at step  16000 :  9.79518943954\n",
      "Average loss at step  18000 :  8.58812181628\n",
      "Average loss at step  20000 :  7.88790464735\n",
      "Nearest to some: these, the, truetype, rico, many, this, walk, improvement,\n",
      "Nearest to six: nine, eight, zero, three, seven, four, five, two,\n",
      "Nearest to or: and, three, six, vs, dasyprocta, the, is, with,\n",
      "Nearest to see: oversaw, and, victoriae, vs, neutrons, write, arrests, baseball,\n",
      "Nearest to only: stresses, mathbf, implicit, wcw, mathematics, agouti, brighton, choices,\n",
      "Nearest to eight: nine, zero, seven, five, six, four, two, three,\n",
      "Nearest to up: dasyprocta, gravitational, tora, categorization, void, a, overview, schedule,\n",
      "Nearest to th: six, microscopy, nine, zero, eight, argo, submit, grandparents,\n",
      "Nearest to while: and, reginae, deliberate, crew, pain, is, printers, amethyst,\n",
      "Nearest to if: dasyprocta, antlia, kyoto, try, studies, suspended, artifacts, fields,\n",
      "Nearest to their: the, his, appointed, its, hello, benefit, winged, inhabited,\n",
      "Nearest to on: in, and, for, with, dasyprocta, agouti, nebulae, blank,\n",
      "Nearest to these: some, piles, group, austria, control, pick, gracious, sense,\n",
      "Nearest to years: fao, tours, backed, convictions, barclay, operatorname, dasyprocta, reginae,\n",
      "Nearest to b: unlimited, and, psi, hashshashin, ages, alongside, jose, d,\n",
      "Nearest to by: in, for, is, was, with, and, from, as,\n",
      "Average loss at step  22000 :  7.28684493244\n",
      "Average loss at step  24000 :  6.94873830879\n",
      "Average loss at step  26000 :  6.70001512957\n",
      "Average loss at step  28000 :  6.17255519617\n",
      "Average loss at step  30000 :  6.20986673534\n",
      "Nearest to some: these, many, the, this, truetype, ramps, rico, walk,\n",
      "Nearest to six: eight, nine, seven, four, three, five, zero, two,\n",
      "Nearest to or: and, akita, vs, dasyprocta, speedup, reginae, five, six,\n",
      "Nearest to see: oversaw, and, akita, victoriae, clockwork, neutrons, arrests, vs,\n",
      "Nearest to only: stresses, mathbf, wcw, implicit, devi, agouti, mathematics, copyright,\n",
      "Nearest to eight: nine, seven, five, six, four, three, zero, two,\n",
      "Nearest to up: gravitational, dasyprocta, aedh, callisto, categorization, a, void, tora,\n",
      "Nearest to th: eight, six, nine, four, five, seven, microscopy, primigenius,\n",
      "Nearest to while: and, reginae, deliberate, or, is, amethyst, but, pain,\n",
      "Nearest to if: dasyprocta, antlia, kyoto, callisto, try, marilyn, though, suspended,\n",
      "Nearest to their: the, his, its, winged, hello, appointed, a, benefit,\n",
      "Nearest to on: in, and, from, for, at, two, of, dasyprocta,\n",
      "Nearest to these: some, many, piles, austria, the, group, gracious, pick,\n",
      "Nearest to years: fao, convictions, tours, backed, began, weeks, two, barclay,\n",
      "Nearest to b: d, unlimited, psi, akita, and, alongside, hashshashin, jose,\n",
      "Nearest to by: in, was, from, with, and, for, as, were,\n",
      "Average loss at step  32000 :  5.8294301796\n",
      "Average loss at step  34000 :  5.87250620019\n",
      "Average loss at step  36000 :  5.69099139905\n",
      "Average loss at step  38000 :  5.27767212915\n",
      "Average loss at step  40000 :  5.49069370103\n",
      "Nearest to some: many, these, this, the, truetype, ramps, walk, rico,\n",
      "Nearest to six: four, seven, eight, three, five, nine, zero, two,\n",
      "Nearest to or: and, akita, dasyprocta, four, vs, three, eight, reginae,\n",
      "Nearest to see: and, oversaw, clockwork, akita, victoriae, neutrons, arrests, vs,\n",
      "Nearest to only: stresses, agouti, mathematics, mathbf, kifl, wcw, devi, amputation,\n",
      "Nearest to eight: nine, seven, four, six, five, three, zero, two,\n",
      "Nearest to up: gravitational, dasyprocta, flesh, a, callisto, bytes, hello, aedh,\n",
      "Nearest to th: eight, seven, zero, six, nine, satellites, five, four,\n",
      "Nearest to while: and, reginae, but, mcduck, amethyst, is, or, deliberate,\n",
      "Nearest to if: dasyprocta, though, antlia, kyoto, although, when, suspended, try,\n",
      "Nearest to their: its, his, the, winged, hello, appointed, recitative, exceptionally,\n",
      "Nearest to on: in, from, at, dasyprocta, with, agouti, for, aveiro,\n",
      "Nearest to these: some, many, piles, which, austria, lemmy, loving, ultrasound,\n",
      "Nearest to years: convictions, alphanumeric, tours, fao, backed, weeks, occuring, centuries,\n",
      "Nearest to b: d, unlimited, UNK, akita, psi, alongside, and, traveller,\n",
      "Nearest to by: was, is, amputation, with, from, in, discard, and,\n",
      "Average loss at step  42000 :  5.27434798539\n",
      "Average loss at step  44000 :  5.29417714393\n",
      "Average loss at step  46000 :  5.28498893511\n",
      "Average loss at step  48000 :  5.04466693044\n",
      "Average loss at step  50000 :  5.1448822695\n",
      "Nearest to some: many, these, several, two, the, this, ramps, all,\n",
      "Nearest to six: eight, four, seven, three, five, nine, one, two,\n",
      "Nearest to or: and, akita, eight, three, dasyprocta, thibetanus, reginae, four,\n",
      "Nearest to see: oversaw, clockwork, arrests, neutrons, akita, victoriae, returned, haredi,\n",
      "Nearest to only: stresses, devi, but, mathematics, usually, mathbf, wcw, playground,\n",
      "Nearest to eight: six, seven, nine, four, five, three, zero, one,\n",
      "Nearest to up: gravitational, down, dasyprocta, flesh, bytes, assertion, lautering, slightest,\n",
      "Nearest to th: eight, six, one, nine, seven, satellites, primigenius, five,\n",
      "Nearest to while: and, but, reginae, mcduck, amethyst, is, or, thibetanus,\n",
      "Nearest to if: when, though, is, although, dasyprocta, thibetanus, kyoto, antlia,\n",
      "Nearest to their: its, his, the, hello, winged, exceptionally, recitative, many,\n",
      "Nearest to on: in, at, from, dasyprocta, two, for, agouti, and,\n",
      "Nearest to these: some, many, piles, loving, two, both, ultrasound, other,\n",
      "Nearest to years: naaman, convictions, alphanumeric, milling, three, four, prism, tours,\n",
      "Nearest to b: d, unlimited, akita, psi, alongside, four, one, eight,\n",
      "Nearest to by: was, be, with, amputation, from, seven, akita, abalone,\n",
      "Average loss at step  52000 :  5.16820070219\n",
      "Average loss at step  54000 :  5.10855407691\n",
      "Average loss at step  56000 :  5.05132590389\n",
      "Average loss at step  58000 :  5.10180755854\n",
      "Average loss at step  60000 :  4.93790665847\n",
      "Nearest to some: many, these, several, all, the, this, callithrix, their,\n",
      "Nearest to six: eight, four, five, seven, three, nine, zero, two,\n",
      "Nearest to or: and, callithrix, thibetanus, akita, dasyprocta, reginae, vs, but,\n",
      "Nearest to see: oversaw, and, clockwork, akita, arrests, victoriae, neutrons, callithrix,\n",
      "Nearest to only: but, stresses, usually, callithrix, kifl, agouti, wcw, devi,\n",
      "Nearest to eight: nine, six, seven, four, five, zero, three, callithrix,\n",
      "Nearest to up: tamarin, down, gravitational, gypsy, slightest, assertion, dasyprocta, bytes,\n",
      "Nearest to th: eight, seven, six, nine, five, zero, four, satellites,\n",
      "Nearest to while: and, but, reginae, or, mcduck, when, amethyst, is,\n",
      "Nearest to if: when, though, is, although, dasyprocta, but, thibetanus, antlia,\n",
      "Nearest to their: its, his, the, her, hello, some, recitative, many,\n",
      "Nearest to on: in, at, michelob, callithrix, dasyprocta, from, agouti, aveiro,\n",
      "Nearest to these: some, many, piles, other, loving, both, which, gracious,\n",
      "Nearest to years: six, weeks, prism, naaman, four, centuries, convictions, alphanumeric,\n",
      "Nearest to b: d, unlimited, akita, alongside, tamarin, austin, microcebus, dasyprocta,\n",
      "Nearest to by: was, with, be, in, initial, abalone, amputation, pungent,\n",
      "Average loss at step  62000 :  4.78037212646\n",
      "Average loss at step  64000 :  4.78756461632\n",
      "Average loss at step  66000 :  4.9742611624\n",
      "Average loss at step  68000 :  4.92488823903\n",
      "Average loss at step  70000 :  4.76994549608\n",
      "Nearest to some: many, these, several, all, the, thaler, their, other,\n",
      "Nearest to six: four, eight, five, seven, three, nine, two, zero,\n",
      "Nearest to or: and, callithrix, akita, thibetanus, dasyprocta, reginae, victoriae, michelob,\n",
      "Nearest to see: oversaw, clockwork, and, akita, neutrons, victoriae, arrests, animism,\n",
      "Nearest to only: but, stresses, dinar, usually, callithrix, kifl, agouti, devi,\n",
      "Nearest to eight: nine, six, seven, four, five, three, zero, callithrix,\n",
      "Nearest to up: tamarin, down, gravitational, dasyprocta, assertion, slightest, gypsy, mirza,\n",
      "Nearest to th: eight, seven, six, five, nine, satellites, one, zero,\n",
      "Nearest to while: and, but, reginae, when, or, mcduck, including, amethyst,\n",
      "Nearest to if: when, though, although, dasyprocta, is, but, thibetanus, arthropods,\n",
      "Nearest to their: its, his, the, her, some, many, hello, exceptionally,\n",
      "Nearest to on: in, at, michelob, callithrix, through, dasyprocta, upon, from,\n",
      "Nearest to these: some, many, other, piles, such, all, both, loving,\n",
      "Nearest to years: naaman, weeks, prism, centuries, six, alphanumeric, milling, wct,\n",
      "Nearest to b: d, unlimited, akita, alongside, UNK, seven, tamarin, austin,\n",
      "Nearest to by: was, be, in, with, initial, amputation, michelob, akita,\n",
      "Average loss at step  72000 :  4.79893363619\n",
      "Average loss at step  74000 :  4.77952981204\n",
      "Average loss at step  76000 :  4.88139371681\n",
      "Average loss at step  78000 :  4.79976967895\n",
      "Average loss at step  80000 :  4.81449643528\n",
      "Nearest to some: many, these, several, all, other, thaler, callithrix, their,\n",
      "Nearest to six: five, four, seven, eight, three, nine, two, zero,\n",
      "Nearest to or: and, callithrix, dasyprocta, akita, reginae, thibetanus, clodius, victoriae,\n",
      "Nearest to see: clockwork, oversaw, akita, victoriae, animism, arrests, neutrons, michelob,\n",
      "Nearest to only: but, dinar, stresses, callithrix, usually, agouti, kifl, clodius,\n",
      "Nearest to eight: seven, six, nine, five, four, three, zero, callithrix,\n",
      "Nearest to up: down, tamarin, gravitational, slightest, assertion, loads, gypsy, mirza,\n",
      "Nearest to th: eight, six, seven, five, nine, satellites, four, three,\n",
      "Nearest to while: and, but, reginae, or, when, mcduck, amethyst, including,\n",
      "Nearest to if: when, though, clodius, although, dasyprocta, then, thibetanus, but,\n",
      "Nearest to their: its, his, the, her, some, many, cegep, exceptionally,\n",
      "Nearest to on: in, at, through, escuela, jawaharlal, upon, michelob, callithrix,\n",
      "Nearest to these: some, many, such, all, other, piles, both, two,\n",
      "Nearest to years: centuries, weeks, prism, naaman, milling, alphanumeric, wct, convictions,\n",
      "Nearest to b: d, unlimited, UNK, akita, alongside, tamarin, austin, albury,\n",
      "Nearest to by: be, was, with, initial, rial, during, putty, abalone,\n",
      "Average loss at step  82000 :  4.80334115136\n",
      "Average loss at step  84000 :  4.79316348433\n",
      "Average loss at step  86000 :  4.76971355021\n",
      "Average loss at step  88000 :  4.69298955178\n",
      "Average loss at step  90000 :  4.76991244769\n",
      "Nearest to some: many, these, several, all, other, callithrix, thaler, thibetanus,\n",
      "Nearest to six: eight, five, seven, four, three, nine, two, zero,\n",
      "Nearest to or: and, callithrix, dasyprocta, thibetanus, but, akita, victoriae, reginae,\n",
      "Nearest to see: tamias, clockwork, victoriae, akita, oversaw, and, neutrons, animism,\n",
      "Nearest to only: but, callithrix, dinar, usually, agouti, clodius, kifl, stresses,\n",
      "Nearest to eight: seven, six, five, nine, four, three, zero, two,\n",
      "Nearest to up: down, tamarin, gravitational, slightest, bytes, dynasty, gypsy, mirza,\n",
      "Nearest to th: eight, six, five, seven, nine, satellites, primigenius, unverifiable,\n",
      "Nearest to while: and, but, when, reginae, or, including, mcduck, although,\n",
      "Nearest to if: when, clodius, though, although, dasyprocta, thibetanus, is, where,\n",
      "Nearest to their: its, his, the, her, some, tamias, many, cegep,\n",
      "Nearest to on: in, at, escuela, michelob, upon, dasyprocta, through, callithrix,\n",
      "Nearest to these: some, many, such, all, both, piles, several, other,\n",
      "Nearest to years: centuries, weeks, naaman, prism, dinar, milling, wct, convictions,\n",
      "Nearest to b: d, unlimited, akita, alongside, tamarin, borrow, one, UNK,\n",
      "Nearest to by: abalone, be, was, with, michelob, during, seven, rial,\n",
      "Average loss at step  92000 :  4.70160227251\n",
      "Average loss at step  94000 :  4.6358970114\n",
      "Average loss at step  96000 :  4.72313550389\n",
      "Average loss at step  98000 :  4.6002921868\n",
      "Average loss at step  100000 :  4.67694851768\n",
      "Nearest to some: many, these, several, all, the, any, their, callithrix,\n",
      "Nearest to six: seven, five, four, eight, three, nine, two, zero,\n",
      "Nearest to or: and, callithrix, dasyprocta, thibetanus, akita, victoriae, clodius, reginae,\n",
      "Nearest to see: gaku, tamias, clockwork, victoriae, and, akita, oversaw, but,\n",
      "Nearest to only: callithrix, dinar, cantos, usually, agouti, clodius, kifl, upanija,\n",
      "Nearest to eight: seven, nine, six, five, four, zero, three, two,\n",
      "Nearest to up: down, tamarin, gravitational, out, them, loads, bytes, him,\n",
      "Nearest to th: six, eight, five, seven, nine, primigenius, intifada, unverifiable,\n",
      "Nearest to while: but, and, when, reginae, although, mcduck, including, where,\n",
      "Nearest to if: when, though, clodius, although, dasyprocta, is, where, thibetanus,\n",
      "Nearest to their: its, his, the, her, some, our, tamias, many,\n",
      "Nearest to on: in, at, through, upon, michelob, callithrix, dasyprocta, escuela,\n",
      "Nearest to these: some, many, such, several, all, both, those, other,\n",
      "Nearest to years: centuries, weeks, naaman, prism, dinar, milling, wct, convictions,\n",
      "Nearest to b: d, unlimited, akita, borrow, austin, alongside, tamarin, psi,\n",
      "Nearest to by: be, was, seven, abalone, initial, akita, been, rial,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-00852b897e85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0mplot_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m   \u001b[0mlow_dim_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mplot_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreverse_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m   \u001b[0mplot_with_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow_dim_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dylanjorgensen/anaconda/lib/python3.5/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \"\"\"\n\u001b[0;32m--> 866\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dylanjorgensen/anaconda/lib/python3.5/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    775\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m     def _tsne(self, P, degrees_of_freedom, n_samples, random_state,\n",
      "\u001b[0;32m/Users/dylanjorgensen/anaconda/lib/python3.5/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, random_state, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'it'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopt_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             print(\"[t-SNE] Error after %d iterations with early \"\n",
      "\u001b[0;32m/Users/dylanjorgensen/anaconda/lib/python3.5/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, objective_error, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, min_error_diff, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mnew_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0minc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dylanjorgensen/anaconda/lib/python3.5/site-packages/scipy/linalg/misc.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(a, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# Differs from numpy only in non-finite handling and the use of blas.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Only use optimized norms if axis and keepdims are not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dylanjorgensen/anaconda/lib/python3.5/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         raise ValueError(\n\u001b[0;32m-> 1033\u001b[0;31m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[1;32m   1034\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Download the data.\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urllib.request.urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "        'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n",
    "\n",
    "\n",
    "# Read the data into a list of strings.\n",
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "  return data\n",
    "\n",
    "words = read_data(filename)\n",
    "print('Data size', len(words))\n",
    "\n",
    "# Step 2: Build the dictionary and replace rare words with UNK token.\n",
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count += 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "del words  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [ skip_window ]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch[i], reverse_dictionary[batch[i]],\n",
    "      '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n",
    "\n",
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "  with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,\n",
    "                     num_sampled, vocabulary_size))\n",
    "\n",
    "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Add variable initializer.\n",
    "  init = tf.initialize_all_variables()\n",
    "\n",
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print(\"Initialized\")\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print(\"Average loss at step \", step, \": \", average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in xrange(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "        log_str = \"Nearest to %s:\" % valid_word\n",
    "        for k in xrange(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = \"%s %s,\" % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING!!! - TSNE Issue Update (Wait for fix)\n",
    "- https://github.com/scikit-learn/scikit-learn/issues/6665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 6: Visualize the embeddings.\n",
    "\n",
    "# def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "#   assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "#   plt.figure(figsize=(18, 18))  #in inches\n",
    "#   for i, label in enumerate(labels):\n",
    "#     x, y = low_dim_embs[i,:]\n",
    "#     plt.scatter(x, y)\n",
    "#     plt.annotate(label,\n",
    "#                  xy=(x, y),\n",
    "#                  xytext=(5, 2),\n",
    "#                  textcoords='offset points',\n",
    "#                  ha='right',\n",
    "#                  va='bottom')\n",
    "\n",
    "#   plt.savefig(filename)\n",
    "\n",
    "# try:\n",
    "#   from sklearn.manifold import TSNE\n",
    "#   import matplotlib.pyplot as plt\n",
    "\n",
    "#   tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "#   plot_only = 500\n",
    "#   low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\n",
    "#   labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "#   plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "# except ImportError:\n",
    "#   print(\"Please install sklearn, matplotlib, and scipy to visualize embeddings.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
