# Goal

- This project was built to demonstrate my ability to work with Language and Sequence Processing
- The objective is to build and train a recurrent neural network with victorian style humor then have it generate new text from what it learned.

# Preprocessing

## Data

What kind of data will I need?
- I will need a large corpus on victorian comedidy. For the sources I have chosen this blog?
- For a word to vec model I will need to gather furn the data into plain text.

How will I scrap this data from a funny blog? 
- For a word to vec model I will need to gather as plain text.
- To do that I will use Pythons beautiful soup module to pull in a specific series of html pages parces them for specific html tags, strip out the formatting then save the text to file. 

How should I sanity check/visualize/verify text information?
- I will sure a uniform data type ()
- I will make a note of the overall size and unique characters


## Embeddings

What is the best way to represent this text?
- Vector representations allow us to represent the relationships that may exist between the individual symbols and words by creating a notion of multidimensional distance.
- As an example this allows our model to learn that the word "cat" and "kitten" are embedded close to one another just based on their distance to similar words.
- I will be using the Word2Vec model that creates this multidimensional word space.

How did you preprocess this data for your model? Why?
- RNNs learn seqences over time, this is how we prepare the data.
- Tensorflow also like sequential information in the following binary format. 

What are text embedding?

What are word representations?

How should I sanity check/visualize/verify the embedding information?
- We can visualize the learned vectors by projecting them down to 2 dimensions using for instance something like the t-SNE dimensionality reduction technique. 


## Squences

What is the best way to feed in data in 



# Model

Why did I pick a ANN over other machine learning options?

Why did I pick an RNN model over other ANN's for this project? 
- Because human written english uses sequences
- Because human written english also requires memory for comprehensions.

Why did I pick TensorFlow? Why the SKlearn abstraction layer?

Why did I define the TensorFlow graph structure the way I did. (Nodes, edges, squence steps and batch size)

What are some of the memory cell options you considered, but passed on?

Why did I choose an LSTM cell memory cell?

Why did choose the parameters I did? (Learning rate)




 
# Evaluation

How did I go about evaluating the fit of the model?
- Tensorboard
- test/train/validation data?
- Accuracy?
- Episodes
- structure size
- Dynamic sequences?

What were other evaluation metrics you considered, but passed on? 
- accuracy
- confusion matrix

What were the results?
- Visualization
 
What did I learn from them? (Adjust parameters)

## Regularization

What techniques did you try for regularization? Finally decide on?

## Conclusion

Did I successed in making something funny?



 
 
 
 
 
 
 