TODO

- Clean out main folder
- Clean question text up in google doc with spellcheck
- Work on HTMl scraping














Goal


Goal
Demonstrations of how the use of natural language processing, sequence text processing and comedic writing can be combine to product computational humor.

Objective
The objective is to build and train a recurrent neural network with victorian style humor then have it generate new text from what it learned.





Data Preprocessing


What kind of data will I need?
I will need a large corpus of text. 
The source I have chosen this blog that archives on victorian comedy.

How will I get this data? 
- I will need to scrape the website then strip it of markup
- To do this I will use Python's “Beautifulsoup” module to read, parse and format the text.

How will I sanity check text data?
- I will first read some random samples of then ensure a uniform data type


Embeddings


What is the best way to represent this text? Why?
Because RNNs can take in sequences we can benefit from from vector (embedded) representations of our text where the position of each word in the vector can represent the relationship that describe meaning.

What embedding format should I choose? 
I simple list world work how to improve my final results use the Word2Vec model which optimizes the multidimensional distance of each word by training by it's own mini-neural network. 

How should I sanity check/visualize/verify the embedding information?
Even though the word embeddings are made from a higher dimensional arrays we can use dimensionality reduction techniques like T-SNE.











Model


What machine learning model should I build?
I will build a deep recurrent neural networks model over other machine learning options because natural requires require both a simplification of exponentially exploding word combinations and a sequential memory for comprehensions.
I was inspired by several several text generation breakthroughs and Andrej Karpathy Paul Graham Generator that uses RNNs.

How deep should I made the network to simplify the word combinations?
Skinny jeans problem.
The first thing I did was list my current and constraints, such as computing power, and data acquired.
The common method for defining a neural network size is over estimate them then use something like to prune the size bac using regularization to tone it back. (Skinny jeans problem)

Why did I choose an LSTM cell memory cell for sequential processing?
Because memory cells are essentially different way if gating information to prevent vanishing gradient problem for this case I just went with the gold standard LSTM.

What programming language/modules should I use?
I will build it in Tensorflow because I believe it will become the leading NN library of the future. This is because of Google's support/adoption.




Pipeline


How will I format and feed the data into my Tensorflow Model?
Tensorflow also like sequential information in the following binary format. 




Evaluation



How did I go about evaluating the fit of the model?
- Tensorboard
- test/train/validation data?
- Accuracy?
- Episodes
- structure size
- Dynamic sequences?

What were other evaluation metrics you considered, but passed on? 
- accuracy
- confusion matrix

What were the results?
- Visualization
 
What did I learn from them? (Adjust parameters)

## Regularization

What techniques did you try for regularization? Finally decide on?

## Conclusion

Did I succeed in making something funny?

