{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- [TensorFlow - Word2Vec Tutorial](https://www.tensorflow.org/versions/r0.11/tutorials/word2vec/index.html)\n",
    "- [TensorFlow - GitHub](https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/examples/tutorials/word2vec/word2vec_basic.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Build a template for a basic regression problem using an recurrent neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1.0   # How much we move our gradient parameters per episode\n",
    "EPISODES = 300         # Total episodes\n",
    "PRINT = 50            # Print info every x itteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "import os\n",
    "\n",
    "# Downloads data\n",
    "path = \"victorian-jokes.txt\"\n",
    "if not os.path.isfile(path):\n",
    "    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/dylanjorgensen/datasets/master/victorian/victorian-jokes.txt\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Battle of the Nile. Two naval officers were disputing as to the importance of Lord Nelson\\'s victories. They wereunable to agree in opin\\n\\nThe Latter. Days of Bonaparte. At the close of the year 1820, Napoleon\\'s health began to fail so as to excite the greatest apprehensions.\\n\\nAn Unrehearsed Stage Effect. A good story is told of a certain actor whose fate it was to represent the inferior personages in the drama, s\\n\\n\" What\\'s in a Name.\" Iremember, says an old writer,a school-fellow of mine who wasa striking instance of the inconvenience ofa remarkableCh\\n\\nThe Blessing of Forgiveness. The brave only know how to forgive; it is the most refined and generous pitch of virtue human nature can arriv\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reads text to string\n",
    "with open(path, 'r') as fp:\n",
    "    raw_txt = fp.read()\n",
    "    \n",
    "raw_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List: Simple List Each Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 121)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of each word\n",
    "words_list = tf.compat.as_str(raw_txt).split()\n",
    "\n",
    "type(words_list), len(words_list) #data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "vocabulary_size = len(words_list)\n",
    "\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuple: Count Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 92)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counts instances\n",
    "import collections\n",
    "\n",
    "# Tuple of (word, instance_count) pairs ranked by most common\n",
    "count = [['UNK', -1]]\n",
    "count.extend(collections.Counter(words_list).most_common(vocabulary_size - 1))\n",
    "\n",
    "type(count), len(count), # count\n",
    "#  ('to', 6),\n",
    "#  ('The', 4),\n",
    "#  ('in', 3),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary: Maps Unique Words to Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dictionary of unique words with an index\n",
    "dictionary = dict()\n",
    "for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "\n",
    "import operator\n",
    "# sorted(dictionary.items(), key=operator.itemgetter(0)) # Sorted by key\n",
    "# sorted(dictionary.items(), key=operator.itemgetter(1)) # Sorted by value\n",
    "\n",
    "#  'A': 52,\n",
    "#  'An': 51,\n",
    "#  'At': 75,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List: Replaces Text to Intigers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empty list and value\n",
    "data = []\n",
    "unk_count = 0\n",
    "\n",
    "# Read through all text\n",
    "for word in words_list:    \n",
    "    if word in dictionary:\n",
    "        index = dictionary[word] # Store the words index number\n",
    "    else:\n",
    "        index = 0  # # Store the int zero - dictionary['UNK']\n",
    "        unk_count += 1 # Keep track of how many unknowns we have\n",
    "    \n",
    "    # Make a list of all our word indexes and unknowns\n",
    "    data.append(index)\n",
    "\n",
    "# Turns our text into a mapping where each word is replaced with a number\n",
    "len(data), # data\n",
    "# [4, 50, 1, 2, 78,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add the new unknowns to our cout tuple\n",
    "count[0][1] = unk_count\n",
    "\n",
    "#  ('to', 6),\n",
    "#  ('The', 4),\n",
    "#  ('in', 3),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data [4, 80, 2, 1, 34, 15, 60, 68, 57, 89] ['The', 'Battle', 'of', 'the', 'Nile.', 'Two', 'naval', 'officers', 'were', 'disputing']\n"
     ]
    }
   ],
   "source": [
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "# Gives us a new reverse dictionary with the numbers as keys\n",
    "{k: reverse_dictionary[k] for k in sorted(reverse_dictionary.keys())[:10]}\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Batch\n",
    "- This function will be called each iteration. \n",
    "- It's built in a way that provides the right data for the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: Build and train a skip-gram model.\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 Battle -> 4 The\n",
      "80 Battle -> 2 of\n",
      "2 of -> 80 Battle\n",
      "2 of -> 1 the\n",
      "1 the -> 34 Nile.\n",
      "1 the -> 2 of\n",
      "34 Nile. -> 15 Two\n",
      "34 Nile. -> 1 the\n"
     ]
    }
   ],
   "source": [
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 0 (Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 1 (Output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look up embeddings for inputs.\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Construct the variables for the NCE loss\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the average NCE loss for the batch.\n",
    "# tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "# time we evaluate the loss.\n",
    "cost = tf.reduce_mean( tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels, num_sampled, vocabulary_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer: SGD\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "# norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "# normalized_embeddings = embeddings / norm\n",
    "# valid_embeddings = tf.nn.embedding_lookup(\n",
    "#   normalized_embeddings, valid_dataset)\n",
    "# similarity = tf.matmul(\n",
    "#   valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Interactive TensorFlow session.\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize all vars\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 1\n",
      "Cost 44.8232\n",
      "Ep: 50\n",
      "Cost 2.55298\n",
      "Ep: 100\n",
      "Cost 2.34584\n",
      "Ep: 150\n",
      "Cost 1.99374\n",
      "Ep: 200\n",
      "Cost 2.15693\n",
      "Ep: 250\n",
      "Cost 2.09207\n",
      "Ep: 300\n",
      "Cost 1.99474\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10\n",
    "\n",
    "# Train & Log\n",
    "for ep in range(EPISODES):\n",
    "    \n",
    "    # Specify our batch size\n",
    "    batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "    batch_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "    \n",
    "    # Iterativly runs our optimizer\n",
    "    sess.run(optimizer, feed_dict=batch_dict)\n",
    "    sess.run(cost, feed_dict=batch_dict)\n",
    "\n",
    "    # Print\n",
    "    if (ep+1) == 1 or (ep+1) % PRINT == 0:\n",
    "        print(\"Ep:\", ep+1)     \n",
    "        print(\"Cost\", sess.run(cost, feed_dict=batch_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING!!! - TSNE Issue Update (Wait for fix)\n",
    "- https://github.com/scikit-learn/scikit-learn/issues/6665"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 6: Visualize the embeddings.\n",
    "\n",
    "# def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "#   assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "#   plt.figure(figsize=(18, 18))  #in inches\n",
    "#   for i, label in enumerate(labels):\n",
    "#     x, y = low_dim_embs[i,:]\n",
    "#     plt.scatter(x, y)\n",
    "#     plt.annotate(label,\n",
    "#                  xy=(x, y),\n",
    "#                  xytext=(5, 2),\n",
    "#                  textcoords='offset points',\n",
    "#                  ha='right',\n",
    "#                  va='bottom')\n",
    "\n",
    "#   plt.savefig(filename)\n",
    "\n",
    "# try:\n",
    "#   from sklearn.manifold import TSNE\n",
    "#   import matplotlib.pyplot as plt\n",
    "\n",
    "#   tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "#   plot_only = 500\n",
    "#   low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only,:])\n",
    "#   labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "#   plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "# except ImportError:\n",
    "#   print(\"Please install sklearn, matplotlib, and scipy to visualize embeddings.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
